{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trainable Embeddings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w8DRjGAHzyG"
      },
      "source": [
        "# Usar keras 2.2.5\n",
        "# conda install -c conda-forge keras=2.2.5"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnLq6p7aHzyK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb00b9ad-4103-483d-b133-d52cd88978b8"
      },
      "source": [
        "!pip install tensorflow==1.14\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install keras==2.2.5\n",
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==1.14 in /usr/local/lib/python3.7/dist-packages (1.14.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.4.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.0.8)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.42.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.13.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (3.17.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.37.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.14) (1.5.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.15.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.2.5) (1.5.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.2.5'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6qXmkzsHzyO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "45d30aac-f04b-456a-ee9b-8bcfbca44508"
      },
      "source": [
        "import numpy as np\n",
        "np.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.19.5'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KnibsN1xUk3"
      },
      "source": [
        "#from keras.datasets import imdb as dataset\n",
        "from keras.datasets import reuters as dataset\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvpowIEOHzyT"
      },
      "source": [
        "# Cargamos y analizamos el dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4ra2gF4HzyU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0dca00d-5a03-4728-a19d-2f78ee4b76a0"
      },
      "source": [
        "# Primer hyperparámetro\n",
        "num_words=30000\n",
        "\n",
        "(training_data, training_targets), (testing_data, testing_targets) = dataset.load_data(num_words=num_words+2)\n",
        "data = np.concatenate((training_data, testing_data), axis=0)\n",
        "targets = np.concatenate((training_targets, testing_targets), axis=0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/datasets/reuters.py:57: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  xs, labels = f['x'], f['y']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYWTLfB1xUlI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f869dbec-2d2b-4f19-d90d-286b93929ef9"
      },
      "source": [
        "# Tengo dos categorías: Sentimiento positivo (1) o sentimiento negativo (0)\n",
        "num_categories = len(np.unique(targets))\n",
        "print(\"Categories:\", np.unique(targets))\n",
        "# Tengo num_words palabras únicas en el vocabulario\n",
        "print(\"Number of unique words:\", len(np.unique(np.hstack(data))))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categories: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45]\n",
            "Number of unique words: 30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lZ-DUxtxUlT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfef4a91-5687-4cc3-beac-3f1646ea1c98"
      },
      "source": [
        "# Longitudes promedio de los comentarios de las películas\n",
        "length = [len(i) for i in data]\n",
        "print(\"Average Review length:\", np.mean(length))\n",
        "print(\"Standard Deviation:\", round(np.std(length)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Review length: 145.96419665122906\n",
            "Standard Deviation: 146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGeDrxcRHzya"
      },
      "source": [
        "# Impresión de comentario preprocesado con su etiqueta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnT33-acxUlZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef92736b-336a-43e0-a9de-da5d7b8eb36f"
      },
      "source": [
        "# Imprimo cometario i'esimo con su clasificación de sentimiento\n",
        "i = 0\n",
        "print(\"Label:\", targets[i])\n",
        "# Las comentarios ya están preprocesados\n",
        "print(data[i])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 3\n",
            "[1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSVq9thIxUlh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db8ebd2c-0d26-4c9a-e02e-8917405facab"
      },
      "source": [
        "# Bajamos diccionario de palabras a indices\n",
        "index = dataset.get_word_index()\n",
        "print([f'{k}:{v}' for k,v in index.items()][:100])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mdbl:10996', 'fawc:16260', 'degussa:12089', 'woods:8803', 'hanging:13796', 'localized:20672', 'sation:20673', 'chanthaburi:20675', 'refunding:10997', 'hermann:8804', 'passsengers:20676', 'stipulate:20677', 'heublein:8352', 'screaming:20713', 'tcby:16261', 'four:185', 'grains:1642', 'broiler:20680', 'wooden:12090', 'wednesday:1220', 'highveld:13797', 'duffour:7593', '0053:20681', 'elections:3914', '270:2563', '271:3551', '272:5113', '273:3552', '274:3400', 'rudman:7975', '276:3401', '277:3478', '278:3632', '279:4309', 'dormancy:9381', 'errors:7247', 'deferred:3086', 'sptnd:20683', 'cooking:8805', 'stratabit:20684', 'designing:16262', 'metalurgicos:20685', 'databank:13798', '300er:20686', 'shocks:20687', 'nawg:7972', 'tnta:20688', 'perforations:20689', 'affiliates:2891', '27p:20690', 'ching:16263', 'china:595', 'wagyu:16264', 'affiliated:3189', 'chino:16265', 'chinh:16266', 'slickline:20692', 'doldrums:13799', 'kids:12092', 'climbed:3028', 'controversy:6693', 'kidd:20693', 'spotty:12093', 'rebel:12639', 'millimetres:9382', 'golden:4007', 'projection:5689', 'stern:12094', \"hudson's:7903\", 'dna:10066', 'dnc:20695', 'hodler:20696', 'lme:2394', 'insolvancy:20697', 'music:13800', 'therefore:1984', 'dns:10998', 'distortions:6959', 'thassos:13801', 'populations:20698', 'meteorologist:8806', 'loss:43', 'exco:9383', 'adventist:20813', 'murchison:16267', 'locked:10999', 'kampala:13802', 'arndt:20699', 'nakasone:1267', 'steinweg:20700', \"india's:3633\", 'wang:3029', 'wane:10067', 'unjust:13803', 'titanium:13804', 'want:850', 'pinto:20701', \"institutes':16268\", 'absolute:7973', 'travel:4677']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrmkTbXyHzyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6e52f2e-c9d9-4ed8-f3b2-e32b64a1c376"
      },
      "source": [
        "# Armo diccionario reverso: de indices a palabras\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
        "print([f'{k}:{v}' for k,v in reverse_index.items()][:100])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['10996:mdbl', '16260:fawc', '12089:degussa', '8803:woods', '13796:hanging', '20672:localized', '20673:sation', '20675:chanthaburi', '10997:refunding', '8804:hermann', '20676:passsengers', '20677:stipulate', '8352:heublein', '20713:screaming', '16261:tcby', '185:four', '1642:grains', '20680:broiler', '12090:wooden', '1220:wednesday', '13797:highveld', '7593:duffour', '20681:0053', '3914:elections', '2563:270', '3551:271', '5113:272', '3552:273', '3400:274', '7975:rudman', '3401:276', '3478:277', '3632:278', '4309:279', '9381:dormancy', '7247:errors', '3086:deferred', '20683:sptnd', '8805:cooking', '20684:stratabit', '16262:designing', '20685:metalurgicos', '13798:databank', '20686:300er', '20687:shocks', '7972:nawg', '20688:tnta', '20689:perforations', '2891:affiliates', '20690:27p', '16263:ching', '595:china', '16264:wagyu', '3189:affiliated', '16265:chino', '16266:chinh', '20692:slickline', '13799:doldrums', '12092:kids', '3028:climbed', '6693:controversy', '20693:kidd', '12093:spotty', '12639:rebel', '9382:millimetres', '4007:golden', '5689:projection', '12094:stern', \"7903:hudson's\", '10066:dna', '20695:dnc', '20696:hodler', '2394:lme', '20697:insolvancy', '13800:music', '1984:therefore', '10998:dns', '6959:distortions', '13801:thassos', '20698:populations', '8806:meteorologist', '43:loss', '9383:exco', '20813:adventist', '16267:murchison', '10999:locked', '13802:kampala', '20699:arndt', '1267:nakasone', '20700:steinweg', \"3633:india's\", '3029:wang', '10067:wane', '13803:unjust', '13804:titanium', '850:want', '20701:pinto', \"16268:institutes'\", '7973:absolute', '4677:travel']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhq6d5MWHzyi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3aac672-e674-4218-a2b5-c337ca3cee19"
      },
      "source": [
        "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in data[1]] )\n",
        "print(data[1])\n",
        "print()\n",
        "print(decoded)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3267, 699, 3434, 2295, 56, 16784, 7511, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 19261, 49, 2295, 13415, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 13415, 2, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12]\n",
            "\n",
            "# generale de banque sa lt genb br and lt heller overseas corp of chicago have each taken 50 pct stakes in factoring company sa belgo factors generale de banque said in a statement it gave no financial details of the transaction sa belgo # turnover in 1986 was 17 5 billion belgian francs reuter 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBL_1v2eHzym"
      },
      "source": [
        "# Padding y formateo de data para entrenar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HifepVsJxUlo"
      },
      "source": [
        "# Hyperparametro - Longitud máxima de comentario\n",
        "maxlen=1000"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb8Mf33exUlu"
      },
      "source": [
        "data = pad_sequences(data,maxlen=maxlen)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq-c12e5xUl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f60cef69-63fd-409e-b593-60888c602ce6"
      },
      "source": [
        "# Verificamos que todos tengan longitud 1000\n",
        "print(len(data[0]))\n",
        "print(np.array([len(d) for d in data]).var())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXBIUZaNxUmD"
      },
      "source": [
        "data=np.array(data)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gwma-IqxUmK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "264dace9-175b-47e8-b8cc-fc4c97e9270d"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11228, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bplIZHWNUXo"
      },
      "source": [
        "# Armar una MLP con one-hot encoding para resolver el problema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ51AMr2Nbok"
      },
      "source": [
        "from keras.layers import Dense\n",
        "from keras.models import Sequential"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTQSmsc7K2jA",
        "outputId": "33d84f8e-08a4-433a-e7d2-c9810850d9f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "maxlen"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV5eg2fDNdtk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f3847e7-eda8-4ed1-8aab-c107682abfc6"
      },
      "source": [
        "# usar maxlen y num_words para calcular la entrada\n",
        "# Utilizar una sola capa\n",
        "model = Sequential()\n",
        "## TODO\n",
        "salida_densa = 1\n",
        "input_shape = (1,num_words*maxlen)\n",
        "model.add(Dense(salida_densa, input_shape=input_shape, activation='softmax'))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXqCIIv6OWe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea37b6ee-184e-44af-e6e7-fe7ff6aaf8ae"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 1, 1)              30000001  \n",
            "=================================================================\n",
            "Total params: 30,000,001\n",
            "Trainable params: 30,000,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-RLYV5QPBEX"
      },
      "source": [
        "## ¿Por que no es viable esta red?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnvzV5wiPKjs"
      },
      "source": [
        "# Armar una MLP usando Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecjdmUczPIVf"
      },
      "source": [
        "from keras.layers import Embedding, Flatten, Dropout\n",
        "from keras import optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPTlDXslPO0o"
      },
      "source": [
        "# Cantidad de palabras totales contando las reservadas\n",
        "nb_words=num_words+3\n",
        "# Tamano del embedding. Es un hiperparámetro y puede modificarlo\n",
        "embed_dim=32\n",
        "salida_capa_densa = #Agregar\n",
        "dropout=0.5 # Hiperparámetro\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding('COMPLETAR', 'COMPLETAR', input_length='COMPLETAR', trainable=True))\n",
        "model.add('CAPA NECESARIA PARA ADAPTAR LAS DIMENSIONES DEL EMBEDDING A UNA CAPA DENSA')\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(salida_capa_densa, activation='AGREGAR'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTs8XKCLPVqX"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6bygoYSP1PW"
      },
      "source": [
        "# MODIFIQUE HYPERPARAMS A GUSTO\n",
        "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "\n",
        "model.compile(loss='COMPLETAR', optimizer=adam, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4tv5kkjPuaj"
      },
      "source": [
        "model.fit(data,targets,batch_size=32,epochs=5,validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNPXfSxWQRI3"
      },
      "source": [
        "# Armar una CNN\n",
        "Abajo hay un ejemplo de arquitectur"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8Yi3_A4V5AS"
      },
      "source": [
        "# _________________________________________________________________\n",
        "# Layer (type)                 Output Shape              Param #   \n",
        "# =================================================================\n",
        "# embedding_12 (Embedding)     (None, 1000, 32)          960096    \n",
        "# _________________________________________________________________\n",
        "# conv1d_7 (Conv1D)            (None, 1000, 64)          14400     \n",
        "# _________________________________________________________________\n",
        "# max_pooling1d_4 (MaxPooling1 (None, 500, 64)           0         \n",
        "# _________________________________________________________________\n",
        "# conv1d_8 (Conv1D)            (None, 500, 128)          57472     \n",
        "# _________________________________________________________________\n",
        "# global_max_pooling1d_4 (Glob (None, 128)               0         \n",
        "# _________________________________________________________________\n",
        "# dropout_4 (Dropout)          (None, 128)               0         \n",
        "# _________________________________________________________________\n",
        "# dense_19 (Dense)             (None, 46)                5934      \n",
        "# ================================================================="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssxu2rPVV_d_"
      },
      "source": [
        "# MODIFIQUE HYPERPARAMS A GUSTO\n",
        "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "\n",
        "model.compile(loss='COMPLETAR', optimizer=adam, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzXyAjAvxUmW"
      },
      "source": [
        "model.fit(data,targets,batch_size=32,epochs=10,validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBZtuDJsM3mK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}