{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "01_REINFORCE_causal.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMYtds5fHw1L",
        "outputId": "87a6afc4-3f09-44b0-927a-a5be9b306add",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_bPajZDHw1N"
      },
      "source": [
        "# Power point ACTOR-CRITIC\n",
        "https://docs.google.com/presentation/d/1vub5elAjRyMpzrhiOcOoqCzmqh0VMn-br5Mr4AU53GE/edit?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUyci_WGJMbZ"
      },
      "source": [
        "# Clase para cálculo de media y varianza de una secuencia\n",
        "from time import time\n",
        "import pandas as pd\n",
        "import gym\n",
        "import numpy as np\n",
        "import moviepy.editor as mpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "import keras.backend as K\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import sklearn\n",
        "import sklearn.preprocessing\n",
        "\n",
        "def format_as_pandas(time_step, obs, preds, actions, rewards, disc_sum_rews, ep_returns, decimals = 3):\n",
        "    df = pd.DataFrame({'step': time_step.reshape(-1)})\n",
        "    df['observation'] = [np.array(r*10**decimals, dtype=int)/(10**decimals) for r in obs]\n",
        "    df['policy_distribution']=[np.array(r*10**decimals, dtype=int)/(10**decimals) for r in preds]\n",
        "    df['sampled_action'] = [np.array(r, dtype=int) for r in actions]\n",
        "\n",
        "    df['rewards']=rewards\n",
        "    df['discounted_sum_rewards']=np.array(disc_sum_rews*10**decimals, dtype=int)/(10**decimals)\n",
        "    df['episode_return']=np.array(ep_returns*10**decimals, dtype=int)/(10**decimals)\n",
        "    return df\n",
        "\n",
        "class BaseAgent:\n",
        "    def __init__(self, ENV, logdir_root='logs', n_experience_episodes=1, gamma=0.999, epochs=1, lr=0.001, hidden_layer_neurons=128, EPISODES=2000, eval_period=50, algorithm='REINFORCE', noise=1.0, gif_to_board=False, fps=50, batch_size=128):\n",
        "        self.hidden_layer_neurons = hidden_layer_neurons\n",
        "        self.batch_size = batch_size\n",
        "        self.fps = fps\n",
        "        self.gif_to_board = gif_to_board\n",
        "        self.noise = noise\n",
        "        self.last_eval = 0\n",
        "        self.best_return = -np.inf\n",
        "        self.eval_period = eval_period\n",
        "        self.writer = None\n",
        "        self.epsilon = 1e-12\n",
        "        self.logdir_root = logdir_root\n",
        "        self.EPISODES = EPISODES\n",
        "        self.n_experience_episodes = n_experience_episodes\n",
        "        self.episode = 0\n",
        "        self.gamma = gamma\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "        self.logdir = self.get_log_name(ENV, algorithm, logdir_root)\n",
        "        self.env = gym.make(ENV)\n",
        "        \n",
        "        if type(self.env.action_space) != gym.spaces.box.Box:\n",
        "            self.nA = self.env.action_space.n\n",
        "        else:\n",
        "            print('Warning: El espacio de acción es continuo')\n",
        "            self.nA = self.env.action_space.shape[0]\n",
        "            self.logdir = self.logdir + '_' +  str(self.noise)\n",
        "            \n",
        "        if type(self.env.observation_space) == gym.spaces.box.Box:\n",
        "            self.nS = self.env.observation_space.shape[0]\n",
        "        else:\n",
        "            print('Warning: El espacio de observación no es continuo')\n",
        "        self.model = self.get_policy_model(lr=lr, hidden_layer_neurons=hidden_layer_neurons, input_shape=[self.nS] ,output_shape=self.nA)\n",
        "        \n",
        "        \n",
        "        state_space_samples = np.array(\n",
        "            [self.env.observation_space.sample() for x in range(10000)])\n",
        "        self.scaler = sklearn.preprocessing.StandardScaler()\n",
        "        self.scaler.fit(state_space_samples)\n",
        "        \n",
        "        self.reset_env()\n",
        "        \n",
        "    def get_policy_model(self, lr=0.001, hidden_layer_neurons = 128, input_shape=[4], output_shape=2):\n",
        "        pass\n",
        "        \n",
        "    def get_log_name(self,ENV, algorithm, logdir_root):\n",
        "        name = logdir_root + '/'\n",
        "        name += ENV + '/' + algorithm + '/'\n",
        "        name += str(self.n_experience_episodes) + '_'\n",
        "        name += str(self.epochs) + '_'\n",
        "        name += str(self.batch_size) + '_'\n",
        "        name += str(self.gamma) + '_'\n",
        "        name += str(self.lr) + '_'  + str(int(time()))\n",
        "        return name\n",
        "    \n",
        "    def reset_env(self):\n",
        "        # Se suma uno a la cantidad de episodios\n",
        "        self.episode += 1\n",
        "        # Se observa el primer estado\n",
        "        self.observation = self.env.reset()\n",
        "        # Se resetea la lista con los rewards\n",
        "        self.reward = []\n",
        "        \n",
        "    def get_experience_episodes(self, return_ts=False):\n",
        "        # Antes de llamar esta función hay que asegurarse de que el env esta reseteado\n",
        "        observations = []\n",
        "        actions = []\n",
        "        predictions = []\n",
        "        rewards = []\n",
        "        discounted_rewards = []\n",
        "        episodes_returns = []\n",
        "        episodes_lenghts = []\n",
        "        time_steps = []\n",
        "        exp_episodes = 0\n",
        "        ts_count = 0\n",
        "        # Juega n_experience_episodes episodios\n",
        "        while exp_episodes < self.n_experience_episodes:\n",
        "            # Obtengo acción\n",
        "            action, action_one_hot, prediction = self.get_action(eval=False)\n",
        "            \n",
        "            # Ejecuto acción\n",
        "            observation, reward, done, info = self.env.step(action)\n",
        "            \n",
        "            # Guardo reward obtenido por acción\n",
        "            self.reward.append(reward)\n",
        "\n",
        "            # Notar que se guarda la observación anterior\n",
        "            observations.append(self.observation)\n",
        "            \n",
        "            actions.append(action_one_hot)\n",
        "            predictions.append(prediction.flatten())\n",
        "            rewards.append(reward)\n",
        "            self.observation = observation\n",
        "            ts_count+=1\n",
        "            time_steps.append(ts_count)\n",
        "            if done:\n",
        "                exp_episodes += 1\n",
        "                discounted_reward = self.get_discounted_rewards(self.reward)\n",
        "                discounted_rewards = np.hstack([discounted_rewards, discounted_reward])\n",
        "                ep_len = len(discounted_reward)\n",
        "                episodes_lenghts.append(ep_len)\n",
        "                episodes_returns = episodes_returns + [discounted_reward[0]]*ep_len\n",
        "                self.last_observation = self.observation\n",
        "                self.reset_env()\n",
        "                ts_count = 0\n",
        "        if return_ts:\n",
        "            return np.array(observations), np.array(actions), np.array(predictions), np.array(discounted_rewards), np.array(rewards), np.array(episodes_returns), np.array(episodes_lenghts), self.last_observation, np.array(time_steps).reshape(-1, 1)\n",
        "        else:\n",
        "            return np.array(observations), np.array(actions), np.array(predictions), np.array(discounted_rewards), np.array(rewards), np.array(episodes_returns), np.array(episodes_lenghts), self.last_observation\n",
        "        \n",
        "    \n",
        "    def log_data(self, episode, loss, ep_len_mean, entropy, rv, nomalized_loss, deltaT, ep_return, critic_loss=None):\n",
        "        if self.writer is None:\n",
        "            self.writer = SummaryWriter(self.logdir)\n",
        "            print(f\"correr en linea de comando: tensorboard --logdir {self.logdir_root}/\")\n",
        "            \n",
        "        print(f'\\rEpisode: {episode}', end='')\n",
        "        self.writer.add_scalar('loss', loss, episode)\n",
        "        self.writer.add_scalar('episode_len', ep_len_mean, episode)\n",
        "        self.writer.add_scalar('entropy', entropy, episode)\n",
        "        self.writer.add_scalar('running_var', rv, episode)\n",
        "        self.writer.add_scalar('episode_return', ep_return, episode)\n",
        "        if nomalized_loss is not None:\n",
        "            self.writer.add_scalar('normalized_loss', nomalized_loss, episode)\n",
        "        self.writer.add_scalar('time', deltaT, episode)\n",
        "        if critic_loss is not None:\n",
        "            self.writer.add_scalar('critic_loss', critic_loss, episode)\n",
        "        if self.episode - self.last_eval >= self.eval_period:\n",
        "            if self.gif_to_board:\n",
        "                obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, frames = self.get_eval_episode(return_frames=self.gif_to_board)\n",
        "            else:\n",
        "                obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len = self.get_eval_episode(return_frames=self.gif_to_board)\n",
        "            if self.best_return <= ep_returns[-1]:\n",
        "                self.model.save(self.logdir + '.hdf5')\n",
        "                print()\n",
        "                print(f'Model on episode {self.episode - 1} improved from {self.best_return} to {ep_returns[-1]}. Saved!')\n",
        "                self.best_return = ep_returns[-1]\n",
        "                if self.gif_to_board:\n",
        "                    video = frames.reshape((1, )+frames.shape)\n",
        "                    gif_name =  self.logdir.replace('logs/', '').replace('/','_') + '_' + str(self.episode) + '_' + str(int(self.best_return*100)/100) \n",
        "                    self.writer.add_video(gif_name, np.rollaxis(video, 4, 2), fps=self.fps)\n",
        "                \n",
        "                \n",
        "            self.writer.add_scalar('eval_episode_steps', len(obs), self.episode)\n",
        "            self.writer.add_scalar('eval_episode_return', ep_returns[-1], episode)\n",
        "            self.last_eval = self.episode\n",
        "            \n",
        "    def get_eval_episode(self, gif_name=None, fps=50, return_frames=False):\n",
        "        frames=[]\n",
        "        self.reset_env()\n",
        "        observations = []\n",
        "        actions = []\n",
        "        predictions = []\n",
        "        rewards = []\n",
        "        discounted_rewards = []\n",
        "        episodes_returns = []\n",
        "        episodes_lenghts = []\n",
        "        exp_episodes = 0\n",
        "        if gif_name is not None or return_frames:\n",
        "            frames.append(self.env.render(mode = 'rgb_array'))\n",
        "        while True:\n",
        "            # Juega episodios hasta juntar un tamaño de buffer mínimo\n",
        "            action, action_one_hot, prediction = self.get_action(eval=True)\n",
        "            \n",
        "            observation, reward, done, info = self.env.step(action)\n",
        "            self.reward.append(reward)\n",
        "\n",
        "            # Notar que se guarda la observación anterior\n",
        "            observations.append(self.observation)\n",
        "            actions.append(action_one_hot)\n",
        "            predictions.append(prediction.flatten())\n",
        "            rewards.append(reward)\n",
        "            self.observation = observation\n",
        "            if gif_name is not None or return_frames:\n",
        "                frames.append(self.env.render(mode = 'rgb_array'))\n",
        "            if done:\n",
        "                exp_episodes += 1\n",
        "                discounted_reward = self.get_discounted_rewards(self.reward)\n",
        "                discounted_rewards = np.hstack([discounted_rewards, discounted_reward])\n",
        "                ep_len = len(discounted_reward)\n",
        "                episodes_lenghts.append(ep_len)\n",
        "                episodes_returns = episodes_returns + [discounted_reward[0]]*ep_len\n",
        "                self.reset_env()\n",
        "                if gif_name is not None:\n",
        "                    clip = mpy.ImageSequenceClip(frames, fps=fps)\n",
        "                    clip.write_gif(gif_name, fps=fps, verbose=False, logger=None)\n",
        "                if return_frames:\n",
        "                    return np.array(observations), np.array(actions), np.array(predictions), np.array(discounted_rewards), np.array(rewards), np.array(episodes_returns), np.array(episodes_lenghts), np.array(frames)\n",
        "                return np.array(observations), np.array(actions), np.array(predictions), np.array(discounted_rewards), np.array(rewards), np.array(episodes_returns), np.array(episodes_lenghts)\n",
        "            \n",
        "\n",
        "class RunningVariance:\n",
        "    # Keeps a running estimate of variance\n",
        "\n",
        "    def __init__(self):\n",
        "        self.m_k = None\n",
        "        self.s_k = None\n",
        "        self.k = None\n",
        "\n",
        "    def add(self, x):\n",
        "        if not self.m_k:\n",
        "            self.m_k = x\n",
        "            self.s_k = 0\n",
        "            self.k = 0\n",
        "        else:\n",
        "            old_mk = self.m_k\n",
        "            self.k += 1\n",
        "            self.m_k += (x - self.m_k) / self.k\n",
        "            self.s_k += (x - old_mk) * (x - self.m_k)\n",
        "\n",
        "    def get_variance(self, epsilon=1e-12):\n",
        "        return self.s_k / (self.k - 1 + epsilon) + epsilon\n",
        "    \n",
        "    def get_mean(self):\n",
        "        return self.m_k"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2ifMAPXHw1Q"
      },
      "source": [
        "from time import time\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "import keras.backend as K\n",
        "import numpy as np"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jQsyV3NHw1R"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "import os\n",
        "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display._obj._screen)\n",
        "import moviepy.editor as mpy"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kiVlbd1Hw1T"
      },
      "source": [
        "class ReinforceAgent(BaseAgent):\n",
        "    # def __init__(self):\n",
        "    def get_policy_model(self, lr=0.001, hidden_layer_neurons = 128, input_shape=[4], output_shape=2):\n",
        "        ## Defino métrica - loss sin el retorno multiplicando\n",
        "        def loss_metric(y_true, y_pred):\n",
        "            y_true_norm = K.sign(y_true)\n",
        "            return K.categorical_crossentropy(y_true_norm, y_pred)\n",
        "        model = Sequential()\n",
        "        model.add(Dense(hidden_layer_neurons, input_shape=input_shape, activation='relu'))\n",
        "        model.add(Dense(output_shape, activation='softmax'))\n",
        "        ## Por que la categorical_crossentropy funciona ok?\n",
        "        model.compile(Adam(lr), loss=['categorical_crossentropy'], metrics=[loss_metric])\n",
        "        return model\n",
        "    \n",
        "    def get_action(self, eval=False):\n",
        "        p = self.model.predict([self.observation.reshape(1, self.nS)])\n",
        "        if eval is False:\n",
        "            action = np.random.choice(self.nA, p=p[0]) #np.nan_to_num(p[0])\n",
        "        else:\n",
        "            action = np.argmax(p[0])\n",
        "        action_one_hot = np.zeros(self.nA)\n",
        "        action_one_hot[action] = 1\n",
        "        return action, action_one_hot, p\n",
        "    \n",
        "    def get_entropy(self, preds, epsilon=1e-12):\n",
        "        entropy = np.mean(-np.sum(np.log(preds+epsilon)*preds, axis=1)/np.log(self.nA))\n",
        "        return entropy\n",
        "    \n",
        "    def get_discounted_rewards(self, r):\n",
        "        # Por si es una lista\n",
        "        r = np.array(r, dtype=float)\n",
        "        \"\"\"Take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "        discounted_r = np.zeros_like(r)\n",
        "        running_add = 0\n",
        "        for t in reversed(range(0, r.size)):\n",
        "            running_add = running_add * self.gamma + r[t]\n",
        "            discounted_r[t] = running_add\n",
        "        return discounted_r "
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AYAjmFIJ-ck",
        "outputId": "980020a7-a248-43cf-d08f-0f246d6f7396",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        }
      },
      "source": [
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]\n",
        "import gym\n",
        "env = gym.make(\"LunarLander-v2\")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-1794a3cd1f03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip3 install gym[Box_2D]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LunarLander-v2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# reset/step. Set _gym_disable_underscore_compat = True on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'gym.envs.box2d' has no attribute 'LunarLander'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aNvUa8RHw1V",
        "outputId": "9ee391f8-fe44-490b-b3ef-0aa66e7fd1e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "# reinforce_agent = ReinforceAgent('CartPole-v1', n_experience_episodes=1, EPISODES=500, eval_period=20, epochs=1, lr=0.001, algorithm='REINFORCE_CAUSAL',\n",
        "#                                  fps=20, gif_to_board=True)\n",
        "# reinforce_agent = ReinforceAgent('Acrobot-v1', n_experience_episodes=1, EPISODES=2000, epochs=1, lr=0.001, algorithm='REINFORCE_CAUSAL', fps=25,\n",
        "#                              gif_to_board=True)\n",
        "reinforce_agent = ReinforceAgent('LunarLander-v2', n_experience_episodes=50, EPISODES=2000, epochs=5, lr=0.0001, algorithm='REINFORCE_CAUSAL', \n",
        "                                 gif_to_board=True)\n",
        "\n",
        "initial_time = time()\n",
        "running_variance = RunningVariance()\n",
        "\n",
        "\n",
        "while reinforce_agent.episode < reinforce_agent.EPISODES:\n",
        "    obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, last_obs = reinforce_agent.get_experience_episodes()\n",
        "    \n",
        "    for dr in disc_sum_rews:\n",
        "        running_variance.add(dr)\n",
        "\n",
        "    pseudolabels = actions*disc_sum_rews.reshape(-1, 1)\n",
        "\n",
        "    history = reinforce_agent.model.fit(obs, pseudolabels, verbose=0, epochs=reinforce_agent.epochs, batch_size=128)\n",
        "    \n",
        "    reinforce_agent.log_data(reinforce_agent.episode, \n",
        "                      history.history['loss'][0], \n",
        "                      np.mean(ep_len), \n",
        "                      reinforce_agent.get_entropy(preds), \n",
        "                      running_variance.get_variance(), \n",
        "                      history.history['loss_metric'][0], \n",
        "                      time() - initial_time, np.mean(ep_returns[-1]))\n",
        "    \n",
        "reinforce_agent.writer.close()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-272b959e76a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#                              gif_to_board=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m reinforce_agent = ReinforceAgent('LunarLander-v2', n_experience_episodes=50, EPISODES=2000, epochs=5, lr=0.0001, algorithm='REINFORCE_CAUSAL', \n\u001b[0;32m----> 6\u001b[0;31m                                  gif_to_board=True)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0minitial_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-7744edd64a8a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ENV, logdir_root, n_experience_episodes, gamma, epochs, lr, hidden_layer_neurons, EPISODES, eval_period, algorithm, noise, gif_to_board, fps, batch_size)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_log_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdir_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# reset/step. Set _gym_disable_underscore_compat = True on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'gym.envs.box2d' has no attribute 'LunarLander'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2JevpGyHw1X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}