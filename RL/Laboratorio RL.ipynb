{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "Laboratorio.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "00a7d66a-5517-47d6-873d-5d0c56c44a8f"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "id": "00a7d66a-5517-47d6-873d-5d0c56c44a8f",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LThjlKF9XGCY"
      },
      "source": [
        "import gym\n",
        "\n",
        "LEFT = 0\n",
        "DOWN = 1\n",
        "RIGHT = 2\n",
        "UP = 3\n",
        "\n",
        "policy_1 = {\n",
        "    0: RIGHT,\n",
        "    1: RIGHT, \n",
        "    2: RIGHT,\n",
        "    3: DOWN, \n",
        "    4: RIGHT,\n",
        "    5: RIGHT,\n",
        "    6: RIGHT,\n",
        "    7: DOWN,\n",
        "    9: RIGHT,\n",
        "    10: RIGHT,\n",
        "}\n",
        "\n",
        "policy_2 = {\n",
        "    0: DOWN,\n",
        "    1: DOWN, \n",
        "    2: DOWN,\n",
        "    3: DOWN, \n",
        "    4: RIGHT,\n",
        "    5: DOWN,\n",
        "    6: DOWN,\n",
        "    7: DOWN,\n",
        "    9: RIGHT,\n",
        "    10: RIGHT,\n",
        "}\n",
        "\n",
        "policy_3 = {\n",
        "    0: RIGHT,\n",
        "    1: RIGHT, \n",
        "    2: RIGHT,\n",
        "    3: DOWN, \n",
        "    4: RIGHT,\n",
        "    5: DOWN,\n",
        "    6: LEFT,\n",
        "    7: LEFT,\n",
        "    9: RIGHT,\n",
        "    10: RIGHT,\n",
        "}\n",
        "\n",
        "policy_4 = {\n",
        "    0: DOWN,\n",
        "    1: LEFT, \n",
        "    2: LEFT,\n",
        "    3: LEFT, \n",
        "    4: DOWN,\n",
        "    5: LEFT,\n",
        "    6: LEFT,\n",
        "    7: LEFT,\n",
        "    9: LEFT,\n",
        "    10: LEFT,\n",
        "}\n",
        "\n",
        "class RewardWrapper(gym.RewardWrapper):\n",
        "    def __init__(self, env, step_penalty=0.01):\n",
        "        self.step_penalty = step_penalty\n",
        "        super().__init__(env)\n",
        "    \n",
        "    def reward(self, rew):\n",
        "        # modify rew\n",
        "        if rew == 0:\n",
        "            rew = -self.step_penalty\n",
        "        return rew\n",
        "    \n",
        "class ResetWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "    \n",
        "    def reset(self, start_pos=0):\n",
        "        super().reset()\n",
        "        self.env.env.env.s = start_pos\n",
        "        return start_pos\n",
        "\n",
        "def get_frozenlake_env(is_slippery, step_penalty=0.01, custom_map = ['SFFF', 'FFFF', 'HFFG']):\n",
        "    env = gym.make(\"FrozenLake-v0\", desc=custom_map, is_slippery=is_slippery)\n",
        "    env = RewardWrapper(env, step_penalty=step_penalty)\n",
        "    env = ResetWrapper(env)\n",
        "    return env"
      ],
      "id": "LThjlKF9XGCY",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bfd96d6-eaf8-4f0c-8dca-614b17e227c6"
      },
      "source": [
        "#from frozenlake_helper import get_frozenlake_env, policy_1, policy_2, policy_3, policy_4\n",
        "import numpy as np"
      ],
      "id": "1bfd96d6-eaf8-4f0c-8dca-614b17e227c6",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "129b6dad-29ba-47ff-8e36-d47318c8c6d4"
      },
      "source": [
        "LEFT = 0\n",
        "DOWN = 1\n",
        "RIGHT = 2\n",
        "UP = 3"
      ],
      "id": "129b6dad-29ba-47ff-8e36-d47318c8c6d4",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b3573bf-2ae2-438a-895b-189809a7cf36"
      },
      "source": [
        "policies = [policy_1, policy_2, policy_3, policy_4]"
      ],
      "id": "8b3573bf-2ae2-438a-895b-189809a7cf36",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e9a415b-4ef5-42ef-944d-636b8b9d49db"
      },
      "source": [
        "# Instancio entorno"
      ],
      "id": "7e9a415b-4ef5-42ef-944d-636b8b9d49db"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "076f36b7-2df9-42e5-8bed-93df5db66c24"
      },
      "source": [
        "env = get_frozenlake_env(is_slippery=False, step_penalty=0)"
      ],
      "id": "076f36b7-2df9-42e5-8bed-93df5db66c24",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb33328d-5e53-4e85-9353-b2f60fc77996"
      },
      "source": [
        "### Algunas pruebas para familiarizarse con los entornos de openAI"
      ],
      "id": "fb33328d-5e53-4e85-9353-b2f60fc77996"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1728a58b-e8df-4675-99b8-0878170a70c3",
        "outputId": "3beeea0c-8375-44aa-b86c-5e5e050da345",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "start_position = 1\n",
        "env.reset(start_position)\n",
        "env.render()\n",
        "\n",
        "# Genéro acción y recibo siguiente estado, recompenza y si terminó el episodio\n",
        "obs, reward, done, info = env.step(DOWN)\n",
        "print()\n",
        "print(obs, reward, done, info)\n",
        "print()\n",
        "env.render()"
      ],
      "id": "1728a58b-e8df-4675-99b8-0878170a70c3",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FFFF\n",
            "HFFG\n",
            "\n",
            "5 0 False {'prob': 1.0}\n",
            "\n",
            "  (Down)\n",
            "SFFF\n",
            "F\u001b[41mF\u001b[0mFF\n",
            "HFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c2d3773-0f96-4044-add0-49d7aed1eda9",
        "outputId": "6fbd2995-d31b-4750-a8a2-d0422c2964be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env.step(RIGHT)\n",
        "print(obs, reward, done, info)\n",
        "env.render()"
      ],
      "id": "6c2d3773-0f96-4044-add0-49d7aed1eda9",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 0 False {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "FF\u001b[41mF\u001b[0mF\n",
            "HFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a392df31-037f-43ed-b10a-ba49d2665e6c"
      },
      "source": [
        "# Armar una función que corra un episodio completo y devuelta el retorno acumulado\n",
        "\n",
        "Recibe el entorno, la política y la posición inicial del robot\n",
        "Devuelve el retorno (suma de los rewards)"
      ],
      "id": "a392df31-037f-43ed-b10a-ba49d2665e6c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ef02bab-d5ab-4d84-a660-d7434719f7d0"
      },
      "source": [
        "def get_discounted_return(r, gamma=0.9):\n",
        "    # Por si es una lista\n",
        "    r = np.array(r, dtype=float)\n",
        "    \"\"\"Take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "    discounted_r = np.zeros_like(r)\n",
        "    running_add = 0\n",
        "    for t in reversed(range(0, r.size)):\n",
        "        running_add = running_add * gamma + r[t]\n",
        "        discounted_r[t] = running_add\n",
        "    return discounted_r[0]\n",
        "\n",
        "def run_episode(env, policy, start_pos, gamma=1.0):\n",
        "    obs = env.reset(start_pos)\n",
        "    done = False\n",
        "    total_return = []\n",
        "    rewards = []\n",
        "    while not done:\n",
        "        action = policy[obs]\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        rewards.append(reward)\n",
        "    return get_discounted_return(rewards, gamma)"
      ],
      "id": "2ef02bab-d5ab-4d84-a660-d7434719f7d0",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bce3936a-8f3b-4169-9d2c-c3e7cc7b1e83"
      },
      "source": [
        "# Ejercicio 1:\n",
        "- Entorno determinístico\n",
        "- Penalidad del paso = 0 \n",
        "- Sin discount\n",
        "- Partiendo de posición 0: (0,0)"
      ],
      "id": "bce3936a-8f3b-4169-9d2c-c3e7cc7b1e83"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96ae78be-86d2-4d6f-a07d-528745037400"
      },
      "source": [
        "start_pos = 0\n",
        "step_penalty = 0"
      ],
      "id": "96ae78be-86d2-4d6f-a07d-528745037400",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9895e6d7-bbc6-42bc-a29d-396964423d58"
      },
      "source": [
        "env = get_frozenlake_env(False, step_penalty=step_penalty)"
      ],
      "id": "9895e6d7-bbc6-42bc-a29d-396964423d58",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4bf2f8d-d66d-4590-bf3c-cb6e75585a8a"
      },
      "source": [
        "### Evaluar las distintas políticas pariendo desde la posición 0"
      ],
      "id": "f4bf2f8d-d66d-4590-bf3c-cb6e75585a8a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebcc84e5-5c94-4658-a582-716ce7a9d828",
        "outputId": "d0c824a5-c577-417e-d329-8237356ed718",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i, policy in enumerate(policies):\n",
        "    r = run_episode(env, policy=policy, start_pos=start_pos)\n",
        "    print(f'Return policy_{i+1}:', r)"
      ],
      "id": "ebcc84e5-5c94-4658-a582-716ce7a9d828",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return policy_1: 1.0\n",
            "Return policy_2: 1.0\n",
            "Return policy_3: 1.0\n",
            "Return policy_4: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "538a9717-3d46-4f63-815d-a5e9ff885d59"
      },
      "source": [
        "# Ejercicio 2:\n",
        "- Entorno determinístico\n",
        "- Penalidad del paso = 0.01 \n",
        "- Sin discount\n",
        "- Partiendo de posición 0: (0,0)"
      ],
      "id": "538a9717-3d46-4f63-815d-a5e9ff885d59"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe89538d-856e-43df-a55c-54c6f93ec107"
      },
      "source": [
        "start_pos = 0\n",
        "step_penalty = 0.01\n",
        "env = get_frozenlake_env(False, step_penalty=step_penalty)"
      ],
      "id": "fe89538d-856e-43df-a55c-54c6f93ec107",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "677b88a2-afe4-4ee7-9697-bce68914c966",
        "outputId": "1707650a-c0cd-4b25-8e4e-e5ba5cdd1107",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i, policy in enumerate(policies):\n",
        "    r = run_episode(env, policy=policy, start_pos=start_pos)\n",
        "    print(f'Return policy_{i+1}:', r)"
      ],
      "id": "677b88a2-afe4-4ee7-9697-bce68914c966",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return policy_1: 0.96\n",
            "Return policy_2: 0.96\n",
            "Return policy_3: 0.9199999999999999\n",
            "Return policy_4: -0.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4233c782-f0d6-4e4d-b13a-c5ee71c4c43c"
      },
      "source": [
        "# Ejercicio 3:\n",
        "- Entorno determinístico\n",
        "- Penalidad del paso = 0 \n",
        "- gamma = 0.9\n",
        "- Partiendo de posición 0: (0,0)"
      ],
      "id": "4233c782-f0d6-4e4d-b13a-c5ee71c4c43c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32fbbef9-0f38-4ad8-acdf-bebccff15590"
      },
      "source": [
        "### En este entorne debe crear la función get_discounted_return y modificar run_episode acorde"
      ],
      "id": "32fbbef9-0f38-4ad8-acdf-bebccff15590"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97f17045-9207-495f-8e21-70204829ea54"
      },
      "source": [
        "def get_discounted_return(r, gamma=0.9):\n",
        "    # Por si es una lista\n",
        "    r = np.array(r, dtype=float)\n",
        "    \"\"\"Take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "    discounted_r = np.zeros_like(r)\n",
        "    running_add = 0\n",
        "    for t in reversed(range(0, r.size)):\n",
        "        running_add = running_add * gamma + r[t]\n",
        "        discounted_r[t] = running_add\n",
        "    return discounted_r[0]"
      ],
      "id": "97f17045-9207-495f-8e21-70204829ea54",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c2c3d63-1f51-4916-904c-3f5893aec628"
      },
      "source": [
        "def run_episode(env, policy, start_pos, gamma=1.0):\n",
        "    obs = env.reset(start_pos)\n",
        "    done = False\n",
        "    rewards = []\n",
        "    while not done:\n",
        "        action = policy[obs]\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        rewards.append(reward)\n",
        "    return get_discounted_return(rewards, gamma)"
      ],
      "id": "2c2c3d63-1f51-4916-904c-3f5893aec628",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f056ec10-825a-4cb1-91e9-e6f8ec3daea8"
      },
      "source": [
        "start_pos = 0\n",
        "step_penalty = 0\n",
        "gamma = 0.9\n",
        "env = get_frozenlake_env(False, step_penalty=step_penalty)"
      ],
      "id": "f056ec10-825a-4cb1-91e9-e6f8ec3daea8",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb824f01-91e9-41ca-9518-9f6e7b3afa85",
        "outputId": "f4e26495-f9e4-4158-a356-36b5e75c0835",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i, policy in enumerate(policies):\n",
        "    r = run_episode(env, policy=policy, start_pos=start_pos, gamma=gamma)\n",
        "    print(f'Return policy_{i+1}:', r)"
      ],
      "id": "cb824f01-91e9-41ca-9518-9f6e7b3afa85",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return policy_1: 0.6561000000000001\n",
            "Return policy_2: 0.6561000000000001\n",
            "Return policy_3: 0.43046721000000016\n",
            "Return policy_4: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c4c016c-ecd9-42bb-ba70-ab1d1d73f71a"
      },
      "source": [
        "# Ejercicio 4:\n",
        "- Entorno aleatorio\n",
        "- Penalidad del paso = 0 \n",
        "- gamma = 1.0 (sin discount)\n",
        "- Partiendo de posición 0: (0,0)"
      ],
      "id": "4c4c016c-ecd9-42bb-ba70-ab1d1d73f71a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bd98e06-72b8-47d1-9407-f9ae8b800f8c"
      },
      "source": [
        "start_pos = 0\n",
        "step_penalty = 0\n",
        "gamma = 1.0\n",
        "env = get_frozenlake_env(True, step_penalty=step_penalty)"
      ],
      "id": "0bd98e06-72b8-47d1-9407-f9ae8b800f8c",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d2ecfbb-2855-4d17-bde2-0d58005d6dc4"
      },
      "source": [
        "### En este caso cuando se decide una acción, el agente se moverá hacia el lugar indicado con una probabilidad de 0.33, y se moverá hacia cualquiera de los costados con probabilidad 0.33\n",
        "\n",
        "### Cada vez que corra un episodio obtendrá un resultado diferente, intentelo"
      ],
      "id": "4d2ecfbb-2855-4d17-bde2-0d58005d6dc4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cae2be4b-a22e-46e6-b33a-87292846384c",
        "outputId": "7bbbcb28-ac2c-465a-fb8a-b7296c55f182",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Correr varias veces y ver que el resultado cambia con cada iteración\n",
        "for i, policy in enumerate(policies):\n",
        "    r = run_episode(env, policy=policy, start_pos=start_pos, gamma=gamma)\n",
        "    print(f'Return policy_{i+1}:', r)"
      ],
      "id": "cae2be4b-a22e-46e6-b33a-87292846384c",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return policy_1: 1.0\n",
            "Return policy_2: 0.0\n",
            "Return policy_3: 1.0\n",
            "Return policy_4: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51be3603-4bf6-46ec-aa79-c34ac4a5741f"
      },
      "source": [
        "### Armar una función que corra el episodio N veces y devuleva los retornos, la media y desvio"
      ],
      "id": "51be3603-4bf6-46ec-aa79-c34ac4a5741f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eeb8417-521f-4d00-a18d-2110c0e5584d"
      },
      "source": [
        "def get_expected_return(env, policy, N=5000, start_pos=0, gamma=1.0):\n",
        "    rewards = []\n",
        "    \n",
        "    for i in range(N):\n",
        "        reward = run_episode(env, policy, start_pos=start_pos, gamma=gamma)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    return rewards, np.mean(rewards), np.std(rewards)"
      ],
      "id": "0eeb8417-521f-4d00-a18d-2110c0e5584d",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be7cb54a-5107-4012-9505-c797eb032f37",
        "outputId": "d5a09b07-a655-48bd-db88-a1ca1c76a286",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i, policy in enumerate(policies):\n",
        "    r = get_expected_return(env, policy, N=10_000, start_pos=start_pos, gamma=gamma)\n",
        "    print(f'Return policy_{i+1}:', r[1], r[2])"
      ],
      "id": "be7cb54a-5107-4012-9505-c797eb032f37",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return policy_1: 0.8041 0.39689191223808024\n",
            "Return policy_2: 0.5436 0.4980954125466324\n",
            "Return policy_3: 0.5797 0.49360704006324707\n",
            "Return policy_4: 0.0 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bdcd127-0745-4d80-8ba8-63e8acbf191f",
        "outputId": "a4f90b2d-6821-440c-9acf-9325ff140e28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i, policy in enumerate(policies):\n",
        "    r = get_expected_return(env, policy, N=10_000, start_pos=start_pos, gamma=gamma)\n",
        "    print(f'Return policy_{i+1}:', r[1], r[2])"
      ],
      "id": "9bdcd127-0745-4d80-8ba8-63e8acbf191f",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return policy_1: 0.8018 0.39864365039468524\n",
            "Return policy_2: 0.5326 0.49893610813409767\n",
            "Return policy_3: 0.5704 0.49501902993723385\n",
            "Return policy_4: 0.0 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a021d25-ae7b-435d-a52a-ef14f187cd29"
      },
      "source": [
        "# Ejercicio 5:\n",
        "- Entorno determinístico\n",
        "- Penalidad del paso = 0 \n",
        "- gamma = 0.9 (sin discount)\n",
        "- Partiendo de posición 0: (0,0)"
      ],
      "id": "3a021d25-ae7b-435d-a52a-ef14f187cd29"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2f9e73d-c231-46fc-a03b-cfda6bada81f"
      },
      "source": [
        "### Calcular la value-function de todas los politicas"
      ],
      "id": "e2f9e73d-c231-46fc-a03b-cfda6bada81f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63c0f9ca-a4ab-49e3-81dd-4a519f91c46a"
      },
      "source": [
        "step_penalty = 0\n",
        "gamma = 0.9\n",
        "env = get_frozenlake_env(False, step_penalty=step_penalty)"
      ],
      "id": "63c0f9ca-a4ab-49e3-81dd-4a519f91c46a",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba2b5f84-bb43-41e7-82be-486f63f74fb5",
        "outputId": "1aa7409a-e232-4774-bd7c-6a142e1fe198",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i, policy in enumerate(policies):\n",
        "    print(f'Return policy_{i+1}:')\n",
        "    for start_pos in range(12):\n",
        "        if start_pos in policy:\n",
        "            r = run_episode(env, policy=policy, start_pos=start_pos, gamma=gamma)\n",
        "            print(start_pos, r)\n",
        "    print()"
      ],
      "id": "ba2b5f84-bb43-41e7-82be-486f63f74fb5",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return policy_1:\n",
            "0 0.6561000000000001\n",
            "1 0.7290000000000001\n",
            "2 0.81\n",
            "3 0.9\n",
            "4 0.7290000000000001\n",
            "5 0.81\n",
            "6 0.9\n",
            "7 1.0\n",
            "9 0.9\n",
            "10 1.0\n",
            "\n",
            "Return policy_2:\n",
            "0 0.6561000000000001\n",
            "1 0.7290000000000001\n",
            "2 0.81\n",
            "3 0.9\n",
            "4 0.7290000000000001\n",
            "5 0.81\n",
            "6 0.9\n",
            "7 1.0\n",
            "9 0.9\n",
            "10 1.0\n",
            "\n",
            "Return policy_3:\n",
            "0 0.43046721000000016\n",
            "1 0.47829690000000014\n",
            "2 0.5314410000000002\n",
            "3 0.5904900000000002\n",
            "4 0.7290000000000001\n",
            "5 0.81\n",
            "6 0.7290000000000001\n",
            "7 0.6561000000000001\n",
            "9 0.9\n",
            "10 1.0\n",
            "\n",
            "Return policy_4:\n",
            "0 0.0\n",
            "1 0.0\n",
            "2 0.0\n",
            "3 0.0\n",
            "4 0.0\n",
            "5 0.0\n",
            "6 0.0\n",
            "7 0.0\n",
            "9 0.0\n",
            "10 0.0\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
