{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "REINFORCE_helper.py",
      "provenance": [],
      "authorship_tag": "ABX9TyOJbFuWlQ/gJmAXfy1IFVV2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LCaravaggio/DeepLearning_ITBA/blob/main/RL/REINFORCE_helper_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ok5D7kCHFM0"
      },
      "source": [
        "# Clase para cálculo de media y varianza de una secuencia\n",
        "from time import time\n",
        "import pandas as pd\n",
        "import gym\n",
        "import numpy as np\n",
        "import moviepy.editor as mpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "import keras.backend as K\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import sklearn\n",
        "import sklearn.preprocessing\n",
        "\n",
        "def format_as_pandas(time_step, obs, preds, actions, rewards, disc_sum_rews, ep_returns, decimals = 3):\n",
        "    df = pd.DataFrame({'step': time_step.reshape(-1)})\n",
        "    df['observation'] = [np.array(r*10**decimals, dtype=int)/(10**decimals) for r in obs]\n",
        "    df['policy_distribution']=[np.array(r*10**decimals, dtype=int)/(10**decimals) for r in preds]\n",
        "    df['sampled_action'] = [np.array(r, dtype=int) for r in actions]\n",
        "\n",
        "    df['rewards']=rewards\n",
        "    df['discounted_sum_rewards']=np.array(disc_sum_rews*10**decimals, dtype=int)/(10**decimals)\n",
        "    df['episode_return']=np.array(ep_returns*10**decimals, dtype=int)/(10**decimals)\n",
        "    return df\n",
        "\n",
        "class BaseAgent:\n",
        "    def __init__(self, ENV, logdir_root='logs', n_experience_episodes=1, gamma=0.999, epochs=1, lr=0.001, hidden_layer_neurons=128, EPISODES=2000, eval_period=50, algorithm='REINFORCE', noise=1.0, gif_to_board=False, fps=50, batch_size=128):\n",
        "        self.hidden_layer_neurons = hidden_layer_neurons\n",
        "        self.batch_size = batch_size\n",
        "        self.fps = fps\n",
        "        self.gif_to_board = gif_to_board\n",
        "        self.noise = noise\n",
        "        self.last_eval = 0\n",
        "        self.best_return = -np.inf\n",
        "        self.eval_period = eval_period\n",
        "        self.writer = None\n",
        "        self.epsilon = 1e-12\n",
        "        self.logdir_root = logdir_root\n",
        "        self.EPISODES = EPISODES\n",
        "        self.n_experience_episodes = n_experience_episodes\n",
        "        self.episode = 0\n",
        "        self.gamma = gamma\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "        self.logdir = self.get_log_name(ENV, algorithm, logdir_root)\n",
        "        self.env = gym.make(ENV)\n",
        "        \n",
        "        if type(self.env.action_space) != gym.spaces.box.Box:\n",
        "            self.nA = self.env.action_space.n\n",
        "        else:\n",
        "            print('Warning: El espacio de acción es continuo')\n",
        "            self.nA = self.env.action_space.shape[0]\n",
        "            self.logdir = self.logdir + '_' +  str(self.noise)\n",
        "            \n",
        "        if type(self.env.observation_space) == gym.spaces.box.Box:\n",
        "            self.nS = self.env.observation_space.shape[0]\n",
        "        else:\n",
        "            print('Warning: El espacio de observación no es continuo')\n",
        "        self.model = self.get_policy_model(lr=lr, hidden_layer_neurons=hidden_layer_neurons, input_shape=[self.nS] ,output_shape=self.nA)\n",
        "        \n",
        "        \n",
        "        state_space_samples = np.array(\n",
        "            [self.env.observation_space.sample() for x in range(10000)])\n",
        "        self.scaler = sklearn.preprocessing.StandardScaler()\n",
        "        self.scaler.fit(state_space_samples)\n",
        "        \n",
        "        self.reset_env()\n",
        "        \n",
        "    def get_policy_model(self, lr=0.001, hidden_layer_neurons = 128, input_shape=[4], output_shape=2):\n",
        "        pass\n",
        "        \n",
        "    def get_log_name(self,ENV, algorithm, logdir_root):\n",
        "        name = logdir_root + '/'\n",
        "        name += ENV + '/' + algorithm + '/'\n",
        "        name += str(self.n_experience_episodes) + '_'\n",
        "        name += str(self.epochs) + '_'\n",
        "        name += str(self.batch_size) + '_'\n",
        "        name += str(self.gamma) + '_'\n",
        "        name += str(self.lr) + '_'  + str(int(time()))\n",
        "        return name\n",
        "    \n",
        "    def reset_env(self):\n",
        "        # Se suma uno a la cantidad de episodios\n",
        "        self.episode += 1\n",
        "        # Se observa el primer estado\n",
        "        self.observation = self.env.reset()\n",
        "        # Se resetea la lista con los rewards\n",
        "        self.reward = []\n",
        "        \n",
        "    def get_experience_episodes(self, return_ts=False):\n",
        "        # Antes de llamar esta función hay que asegurarse de que el env esta reseteado\n",
        "        observations = []\n",
        "        actions = []\n",
        "        predictions = []\n",
        "        rewards = []\n",
        "        discounted_rewards = []\n",
        "        episodes_returns = []\n",
        "        episodes_lenghts = []\n",
        "        time_steps = []\n",
        "        exp_episodes = 0\n",
        "        ts_count = 0\n",
        "        # Juega n_experience_episodes episodios\n",
        "        while exp_episodes < self.n_experience_episodes:\n",
        "            # Obtengo acción\n",
        "            action, action_one_hot, prediction = self.get_action(eval=False)\n",
        "            \n",
        "            # Ejecuto acción\n",
        "            observation, reward, done, info = self.env.step(action)\n",
        "            \n",
        "            # Guardo reward obtenido por acción\n",
        "            self.reward.append(reward)\n",
        "\n",
        "            # Notar que se guarda la observación anterior\n",
        "            observations.append(self.observation)\n",
        "            \n",
        "            actions.append(action_one_hot)\n",
        "            predictions.append(prediction.flatten())\n",
        "            rewards.append(reward)\n",
        "            self.observation = observation\n",
        "            ts_count+=1\n",
        "            time_steps.append(ts_count)\n",
        "            if done:\n",
        "                exp_episodes += 1\n",
        "                discounted_reward = self.get_discounted_rewards(self.reward)\n",
        "                discounted_rewards = np.hstack([discounted_rewards, discounted_reward])\n",
        "                ep_len = len(discounted_reward)\n",
        "                episodes_lenghts.append(ep_len)\n",
        "                episodes_returns = episodes_returns + [discounted_reward[0]]*ep_len\n",
        "                self.last_observation = self.observation\n",
        "                self.reset_env()\n",
        "                ts_count = 0\n",
        "        if return_ts:\n",
        "            return np.array(observations), np.array(actions), np.array(predictions), np.array(discounted_rewards), np.array(rewards), np.array(episodes_returns), np.array(episodes_lenghts), self.last_observation, np.array(time_steps).reshape(-1, 1)\n",
        "        else:\n",
        "            return np.array(observations), np.array(actions), np.array(predictions), np.array(discounted_rewards), np.array(rewards), np.array(episodes_returns), np.array(episodes_lenghts), self.last_observation\n",
        "        \n",
        "    \n",
        "    def log_data(self, episode, loss, ep_len_mean, entropy, rv, nomalized_loss, deltaT, ep_return, critic_loss=None):\n",
        "        if self.writer is None:\n",
        "            self.writer = SummaryWriter(self.logdir)\n",
        "            print(f\"correr en linea de comando: tensorboard --logdir {self.logdir_root}/\")\n",
        "            \n",
        "        print(f'\\rEpisode: {episode}', end='')\n",
        "        self.writer.add_scalar('loss', loss, episode)\n",
        "        self.writer.add_scalar('episode_len', ep_len_mean, episode)\n",
        "        self.writer.add_scalar('entropy', entropy, episode)\n",
        "        self.writer.add_scalar('running_var', rv, episode)\n",
        "        self.writer.add_scalar('episode_return', ep_return, episode)\n",
        "        if nomalized_loss is not None:\n",
        "            self.writer.add_scalar('normalized_loss', nomalized_loss, episode)\n",
        "        self.writer.add_scalar('time', deltaT, episode)\n",
        "        if critic_loss is not None:\n",
        "            self.writer.add_scalar('critic_loss', critic_loss, episode)\n",
        "        if self.episode - self.last_eval >= self.eval_period:\n",
        "            if self.gif_to_board:\n",
        "                obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, frames = self.get_eval_episode(return_frames=self.gif_to_board)\n",
        "            else:\n",
        "                obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len = self.get_eval_episode(return_frames=self.gif_to_board)\n",
        "            if self.best_return <= ep_returns[-1]:\n",
        "                self.model.save(self.logdir + '.hdf5')\n",
        "                print()\n",
        "                print(f'Model on episode {self.episode - 1} improved from {self.best_return} to {ep_returns[-1]}. Saved!')\n",
        "                self.best_return = ep_returns[-1]\n",
        "                if self.gif_to_board:\n",
        "                    video = frames.reshape((1, )+frames.shape)\n",
        "                    gif_name =  self.logdir.replace('logs/', '').replace('/','_') + '_' + str(self.episode) + '_' + str(int(self.best_return*100)/100) \n",
        "                    self.writer.add_video(gif_name, np.rollaxis(video, 4, 2), fps=self.fps)\n",
        "                \n",
        "                \n",
        "            self.writer.add_scalar('eval_episode_steps', len(obs), self.episode)\n",
        "            self.writer.add_scalar('eval_episode_return', ep_returns[-1], episode)\n",
        "            self.last_eval = self.episode\n",
        "            \n",
        "    def get_eval_episode(self, gif_name=None, fps=50, return_frames=False):\n",
        "        frames=[]\n",
        "        self.reset_env()\n",
        "        observations = []\n",
        "        actions = []\n",
        "        predictions = []\n",
        "        rewards = []\n",
        "        discounted_rewards = []\n",
        "        episodes_returns = []\n",
        "        episodes_lenghts = []\n",
        "        exp_episodes = 0\n",
        "        if gif_name is not None or return_frames:\n",
        "            frames.append(self.env.render(mode = 'rgb_array'))\n",
        "        while True:\n",
        "            # Juega episodios hasta juntar un tamaño de buffer mínimo\n",
        "            action, action_one_hot, prediction = self.get_action(eval=True)\n",
        "            \n",
        "            observation, reward, done, info = self.env.step(action)\n",
        "            self.reward.append(reward)\n",
        "\n",
        "            # Notar que se guarda la observación anterior\n",
        "            observations.append(self.observation)\n",
        "            actions.append(action_one_hot)\n",
        "            predictions.append(prediction.flatten())\n",
        "            rewards.append(reward)\n",
        "            self.observation = observation\n",
        "            if gif_name is not None or return_frames:\n",
        "                frames.append(self.env.render(mode = 'rgb_array'))\n",
        "            if done:\n",
        "                exp_episodes += 1\n",
        "                discounted_reward = self.get_discounted_rewards(self.reward)\n",
        "                discounted_rewards = np.hstack([discounted_rewards, discounted_reward])\n",
        "                ep_len = len(discounted_reward)\n",
        "                episodes_lenghts.append(ep_len)\n",
        "                episodes_returns = episodes_returns + [discounted_reward[0]]*ep_len\n",
        "                self.reset_env()\n",
        "                if gif_name is not None:\n",
        "                    clip = mpy.ImageSequenceClip(frames, fps=fps)\n",
        "                    clip.write_gif(gif_name, fps=fps, verbose=False, logger=None)\n",
        "                if return_frames:\n",
        "                    return np.array(observations), np.array(actions), np.array(predictions), np.array(discounted_rewards), np.array(rewards), np.array(episodes_returns), np.array(episodes_lenghts), np.array(frames)\n",
        "                return np.array(observations), np.array(actions), np.array(predictions), np.array(discounted_rewards), np.array(rewards), np.array(episodes_returns), np.array(episodes_lenghts)\n",
        "            \n",
        "\n",
        "class RunningVariance:\n",
        "    # Keeps a running estimate of variance\n",
        "\n",
        "    def __init__(self):\n",
        "        self.m_k = None\n",
        "        self.s_k = None\n",
        "        self.k = None\n",
        "\n",
        "    def add(self, x):\n",
        "        if not self.m_k:\n",
        "            self.m_k = x\n",
        "            self.s_k = 0\n",
        "            self.k = 0\n",
        "        else:\n",
        "            old_mk = self.m_k\n",
        "            self.k += 1\n",
        "            self.m_k += (x - self.m_k) / self.k\n",
        "            self.s_k += (x - old_mk) * (x - self.m_k)\n",
        "\n",
        "    def get_variance(self, epsilon=1e-12):\n",
        "        return self.s_k / (self.k - 1 + epsilon) + epsilon\n",
        "    \n",
        "    def get_mean(self):\n",
        "        return self.m_k"
      ],
      "execution_count": 3,
      "outputs": []
    }
  ]
}