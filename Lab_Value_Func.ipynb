{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "Lab_Value_Func.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc07cc05-1015-47fd-aeff-7a7a6f26a1cd"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "id": "cc07cc05-1015-47fd-aeff-7a7a6f26a1cd",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9kB_sKAF9YY"
      },
      "source": [
        "import gym\n",
        "\n",
        "LEFT = 0\n",
        "DOWN = 1\n",
        "RIGHT = 2\n",
        "UP = 3\n",
        "\n",
        "policy_1 = {\n",
        "    0: RIGHT,\n",
        "    1: RIGHT, \n",
        "    2: RIGHT,\n",
        "    3: DOWN, \n",
        "    4: RIGHT,\n",
        "    5: RIGHT,\n",
        "    6: RIGHT,\n",
        "    7: DOWN,\n",
        "    9: RIGHT,\n",
        "    10: RIGHT,\n",
        "}\n",
        "\n",
        "policy_2 = {\n",
        "    0: DOWN,\n",
        "    1: DOWN, \n",
        "    2: DOWN,\n",
        "    3: DOWN, \n",
        "    4: RIGHT,\n",
        "    5: DOWN,\n",
        "    6: DOWN,\n",
        "    7: DOWN,\n",
        "    9: RIGHT,\n",
        "    10: RIGHT,\n",
        "}\n",
        "\n",
        "policy_3 = {\n",
        "    0: RIGHT,\n",
        "    1: RIGHT, \n",
        "    2: RIGHT,\n",
        "    3: DOWN, \n",
        "    4: RIGHT,\n",
        "    5: DOWN,\n",
        "    6: LEFT,\n",
        "    7: LEFT,\n",
        "    9: RIGHT,\n",
        "    10: RIGHT,\n",
        "}\n",
        "\n",
        "policy_4 = {\n",
        "    0: DOWN,\n",
        "    1: LEFT, \n",
        "    2: LEFT,\n",
        "    3: LEFT, \n",
        "    4: DOWN,\n",
        "    5: LEFT,\n",
        "    6: LEFT,\n",
        "    7: LEFT,\n",
        "    9: LEFT,\n",
        "    10: LEFT,\n",
        "}\n",
        "\n",
        "class RewardWrapper(gym.RewardWrapper):\n",
        "    def __init__(self, env, step_penalty=0.01):\n",
        "        self.step_penalty = step_penalty\n",
        "        super().__init__(env)\n",
        "    \n",
        "    def reward(self, rew):\n",
        "        # modify rew\n",
        "        if rew == 0:\n",
        "            rew = -self.step_penalty\n",
        "        return rew\n",
        "    \n",
        "class ResetWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "    \n",
        "    def reset(self, start_pos=0):\n",
        "        super().reset()\n",
        "        self.env.env.env.s = start_pos\n",
        "        return start_pos\n",
        "\n",
        "def get_frozenlake_env(is_slippery, step_penalty=0.01, custom_map = ['SFFF', 'FFFF', 'HFFG']):\n",
        "    env = gym.make(\"FrozenLake-v0\", desc=custom_map, is_slippery=is_slippery)\n",
        "    env = RewardWrapper(env, step_penalty=step_penalty)\n",
        "    env = ResetWrapper(env)\n",
        "    return env"
      ],
      "id": "m9kB_sKAF9YY",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "861911d8-4a29-4b83-bf00-bbbfdd6be315"
      },
      "source": [
        "#from frozenlake_helper import get_frozenlake_env, policy_1, policy_2, policy_3, policy_4, LEFT, RIGHT, UP, DOWN\n",
        "import numpy as np"
      ],
      "id": "861911d8-4a29-4b83-bf00-bbbfdd6be315",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a81f48fd-876d-4059-bc6e-861c0796284b"
      },
      "source": [
        "policies = [policy_1, policy_2, policy_3, policy_4]"
      ],
      "id": "a81f48fd-876d-4059-bc6e-861c0796284b",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a78871f-17c2-425f-ae7b-979e0d274d65"
      },
      "source": [
        "number_of_states = 12"
      ],
      "id": "4a78871f-17c2-425f-ae7b-979e0d274d65",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72f641b8-68a7-4003-ad41-a81db535d964"
      },
      "source": [
        "def get_discounted_return(r, gamma=0.9):\n",
        "    # Por si es una lista\n",
        "    r = np.array(r, dtype=float)\n",
        "    \"\"\"Take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "    discounted_r = np.zeros_like(r)\n",
        "    running_add = 0\n",
        "    for t in reversed(range(0, r.size)):\n",
        "        running_add = running_add * gamma + r[t]\n",
        "        discounted_r[t] = running_add\n",
        "    return discounted_r[0]\n",
        "\n",
        "def run_episode(env, policy, start_pos, gamma=1.0):\n",
        "    obs = env.reset(start_pos)\n",
        "    done = False\n",
        "    rewards = []\n",
        "    while not done:\n",
        "        action = policy[obs]\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        rewards.append(reward)\n",
        "    return get_discounted_return(rewards, gamma)\n",
        "\n",
        "def get_expected_return(env, policy, N=5000, start_pos=0, gamma=1.0):\n",
        "    rewards = []\n",
        "    for i in range(N):\n",
        "        # Implementar\n",
        "        reward = run_episode(env, policy, start_pos=start_pos, gamma=gamma)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    return rewards, np.mean(rewards), np.std(rewards)"
      ],
      "id": "72f641b8-68a7-4003-ad41-a81db535d964",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5889c31-de8c-44e0-a893-9cc05c7e6ae5"
      },
      "source": [
        "# Ejercicio 1: Muestreo en entorno aleatorio"
      ],
      "id": "d5889c31-de8c-44e0-a893-9cc05c7e6ae5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2547707-8b99-464e-beb7-bc21ab900be7"
      },
      "source": [
        "step_penalty = 0\n",
        "gamma = 1.0\n",
        "is_slippery = True\n",
        "env = get_frozenlake_env(is_slippery, step_penalty=step_penalty)\n",
        "policy = policy_1"
      ],
      "id": "d2547707-8b99-464e-beb7-bc21ab900be7",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1392934b-e770-4e91-80d2-a7b48794c7b3"
      },
      "source": [
        "### Armar una función que devuelva la estimación de la V(s)\n",
        "Recibe:\n",
        "- El entorno (env)\n",
        "- La política (policy)\n",
        "- La cantidad de episodios usados para la estimación\n",
        "\n",
        "Devuelve:\n",
        "- numpy array de longitud 12 con los \"value\" donde la posición indica el estado "
      ],
      "id": "1392934b-e770-4e91-80d2-a7b48794c7b3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55d7aab5-8eb1-4f1c-8e86-76bd4629f582"
      },
      "source": [
        "def estimate_V_sampling(env, policy, N=10_000, gamma=1.0):\n",
        "    Vs_sample = np.zeros(number_of_states)\n",
        "    for start_pos in range(number_of_states):\n",
        "        if start_pos in policy:\n",
        "            # Implementar\n",
        "            _, r_mean, r_std = get_expected_return(env, policy=policy, N=N, start_pos=start_pos, gamma=gamma)\n",
        "            Vs_sample[start_pos] = r_mean\n",
        "    return Vs_sample"
      ],
      "id": "55d7aab5-8eb1-4f1c-8e86-76bd4629f582",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78a3c029-b29f-44ab-9d55-491211e83c43",
        "outputId": "bae21511-48e9-45bf-ae74-f87d0646ab4a"
      },
      "source": [
        "N = 1_000\n",
        "for i, policy in enumerate(policies):\n",
        "    Vs_sample_policy_1 = estimate_V_sampling(env, policy, N=N, gamma=gamma)\n",
        "    print('V(s) para policy', i + 1)\n",
        "    print(Vs_sample_policy_1.reshape(3, 4))\n",
        "    print()"
      ],
      "id": "78a3c029-b29f-44ab-9d55-491211e83c43",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V(s) para policy 1\n",
            "[[0.819 1.    1.    1.   ]\n",
            " [0.595 1.    1.    1.   ]\n",
            " [0.    1.    1.    0.   ]]\n",
            "\n",
            "V(s) para policy 2\n",
            "[[0.535 0.684 0.786 0.864]\n",
            " [0.405 0.701 0.829 0.921]\n",
            " [0.    0.79  0.893 0.   ]]\n",
            "\n",
            "V(s) para policy 3\n",
            "[[0.594 0.751 0.819 0.866]\n",
            " [0.421 0.666 0.816 0.887]\n",
            " [0.    0.774 0.907 0.   ]]\n",
            "\n",
            "V(s) para policy 4\n",
            "[[0.    0.    0.    0.196]\n",
            " [0.    0.    0.    0.403]\n",
            " [0.    0.    0.    0.   ]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0740818e-a11e-4055-812e-cb06ada664ee"
      },
      "source": [
        "# Ejercicio 2: Armar modelos de entorno y recompenza"
      ],
      "id": "0740818e-a11e-4055-812e-cb06ada664ee"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5005b45c-4e3c-496f-aba1-4db145b9bb48"
      },
      "source": [
        "### Para el caso del entorno esto sería un diccionario: \n",
        "- con keys de todos los estados posibles (de 0 a 11)\n",
        "- para cada estado un diccionario con keys de las acciones posibles (LEFT, RIGHT, UP, DOWN)\n",
        "- para cada estado y acción un diccionario que indique el listado de las proximas acciones con sus probabilidades"
      ],
      "id": "5005b45c-4e3c-496f-aba1-4db145b9bb48"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "7aa4a5d3-8d56-4213-86db-f6159d252bc5"
      },
      "source": [
        "por ejemplo:\n",
        "\n",
        "print(transition_model)\n",
        "\n",
        "{0: {'LEFT': {0: 0.6672, 4: 0.3328},\n",
        "  'RIGHT': {4: 0.3333, 0: 0.331, 1: 0.3357},\n",
        "  'UP': {1: 0.3341, 0: 0.6659},\n",
        "  'DOWN': {0: 0.3296, 1: 0.338, 4: 0.3324}},\n",
        " 1: {'LEFT': {1: 0.3227, 0: 0.3362, 5: 0.3411},\n",
        "  'RIGHT': {5: 0.3314, 1: 0.3318, 2: 0.3368},\n",
        "  'UP': {2: 0.3338, 0: 0.3308, 1: 0.3354},\n",
        "  'DOWN': {2: 0.3292, 5: 0.3354, 0: 0.3354}},\n",
        "  \n",
        " ...\n",
        " \n",
        " 11: {'LEFT': {11: 1.0},\n",
        "  'RIGHT': {11: 1.0},\n",
        "  'UP': {11: 1.0},\n",
        "  'DOWN': {11: 1.0}}}\n",
        "  "
      ],
      "id": "7aa4a5d3-8d56-4213-86db-f6159d252bc5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92783bb6-a5cf-494e-b9c4-677c1ff1b788"
      },
      "source": [
        "### Para el caso del modelo de recompenza sería: \n",
        "\n",
        "Igual al anterior solo que el ultimo diccionario contiene los proximos estados con la recompenza de cada uno.\n",
        "\n",
        "En este caso para simplificar el código se guarda:\n",
        "- count: cantidad de veces que entro en ese estado para calcular el reward\n",
        "- total_reward: la suma de los rewards\n",
        "- reward: El que nos interesa (total_reward/count)"
      ],
      "id": "92783bb6-a5cf-494e-b9c4-677c1ff1b788"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "4f764aee-a6e9-4730-8268-b8dbaf67e4c5"
      },
      "source": [
        "por ejemplo:\n",
        "\n",
        "print(reward_model)\n",
        "{0: {'LEFT': {4: {'total_reward': 0, 'count': 3259, 'reward': 0.0},\n",
        "   0: {'total_reward': 0, 'count': 6741, 'reward': 0.0}},\n",
        "  'RIGHT': {4: {'total_reward': 0, 'count': 3329, 'reward': 0.0},\n",
        "   1: {'total_reward': 0, 'count': 3326, 'reward': 0.0},\n",
        "   0: {'total_reward': 0, 'count': 3345, 'reward': 0.0}},\n",
        "  'UP': {1: {'total_reward': 0, 'count': 3293, 'reward': 0.0},\n",
        "   0: {'total_reward': 0, 'count': 6707, 'reward': 0.0}},\n",
        "  'DOWN': {4: {'total_reward': 0, 'count': 3320, 'reward': 0.0},\n",
        "   1: {'total_reward': 0, 'count': 3391, 'reward': 0.0},\n",
        "   0: {'total_reward': 0, 'count': 3289, 'reward': 0.0}}},\n",
        " 1: {'LEFT': {0: {'total_reward': 0, 'count': 3351, 'reward': 0.0},\n",
        "   1: {'total_reward': 0, 'count': 3328, 'reward': 0.0},\n",
        "   5: {'total_reward': 0, 'count': 3321, 'reward': 0.0}},\n",
        "  'RIGHT': {5: {'total_reward': 0, 'count': 3270, 'reward': 0.0},\n",
        "   2: {'total_reward': 0, 'count': 3324, 'reward': 0.0},\n",
        "   1: {'total_reward': 0, 'count': 3406, 'reward': 0.0}},\n",
        "  'UP': {2: {'total_reward': 0, 'count': 3277, 'reward': 0.0},\n",
        "   0: {'total_reward': 0, 'count': 3380, 'reward': 0.0},\n",
        "   1: {'total_reward': 0, 'count': 3343, 'reward': 0.0}},\n",
        "  'DOWN': {2: {'total_reward': 0, 'count': 3335, 'reward': 0.0},\n",
        "   5: {'total_reward': 0, 'count': 3258, 'reward': 0.0},\n",
        "   0: {'total_reward': 0, 'count': 3407, 'reward': 0.0}}},\n",
        "   \n",
        " ...\n",
        " \n",
        " 11: {'LEFT': {11: {'total_reward': 0, 'count': 10000, 'reward': 0.0}},\n",
        "  'RIGHT': {11: {'total_reward': 0, 'count': 10000, 'reward': 0.0}},\n",
        "  'UP': {11: {'total_reward': 0, 'count': 10000, 'reward': 0.0}},\n",
        "  'DOWN': {11: {'total_reward': 0, 'count': 10000, 'reward': 0.0}}}}"
      ],
      "id": "4f764aee-a6e9-4730-8268-b8dbaf67e4c5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "882d5ba0-5ffe-46d2-b050-19ba346357a7"
      },
      "source": [
        "action_to_str = {\n",
        "    LEFT: 'LEFT',\n",
        "    RIGHT: 'RIGHT',\n",
        "    UP: 'UP',\n",
        "    DOWN: 'DOWN'\n",
        "}\n",
        "str_to_action = {\n",
        "    'LEFT': LEFT,\n",
        "    'RIGHT': RIGHT,\n",
        "    'UP': UP,\n",
        "    'DOWN': DOWN\n",
        "}"
      ],
      "id": "882d5ba0-5ffe-46d2-b050-19ba346357a7",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b970d860-261a-4fe0-926b-7e0af1abd8d5"
      },
      "source": [
        "actions = [LEFT, RIGHT, UP, DOWN]\n",
        "\n",
        "N = 500\n",
        "transition_model = {}\n",
        "reward_model = {}\n",
        "\n",
        "for start_pos in range(number_of_states):\n",
        "    # Iteración en todos los estados\n",
        "    if start_pos not in transition_model:\n",
        "        # Inicializo diccionario del estado\n",
        "        transition_model[start_pos] = {}\n",
        "        reward_model[start_pos] = {}\n",
        "    for action in actions:\n",
        "        # Iteración en todas las acciones\n",
        "        action_str = action_to_str[action]\n",
        "        if action not in transition_model[start_pos]:\n",
        "            # inicializo diccinario de la accion\n",
        "            transition_model[start_pos][action_str] = {}\n",
        "            reward_model[start_pos][action_str] = {}\n",
        "        for n in range(N):\n",
        "            # Notar que no corro todo el episodio sino que solo me interesa la próxima acción\n",
        "            env.reset(start_pos)\n",
        "            obs, reward, done, info = env.step(action)\n",
        "\n",
        "            if obs not in transition_model[start_pos][action_str]:\n",
        "                # inicializo diccionario de proximo estado\n",
        "                transition_model[start_pos][action_str][obs] = 0\n",
        "                reward_model[start_pos][action_str][obs] = {}\n",
        "                reward_model[start_pos][action_str][obs]['total_reward'] = 0\n",
        "                reward_model[start_pos][action_str][obs]['count'] = 0\n",
        "                reward_model[start_pos][action_str][obs]['reward'] = 0\n",
        "\n",
        "            # Implementar\n",
        "            transition_model[start_pos][action_str][obs] = transition_model[start_pos][action_str][obs] + 1\n",
        "            reward_model[start_pos][action_str][obs]['total_reward'] = reward_model[start_pos][action_str][obs]['total_reward'] + reward\n",
        "            reward_model[start_pos][action_str][obs]['count'] = reward_model[start_pos][action_str][obs]['count'] + 1\n",
        "            reward_model[start_pos][action_str][obs]['reward'] = reward_model[start_pos][action_str][obs]['total_reward'] / reward_model[start_pos][action_str][obs]['count']\n",
        "\n",
        "# Normalización de modelo\n",
        "for state, actions in transition_model.items():\n",
        "    for action, next_state_count in actions.items():\n",
        "        total_count = 0\n",
        "        for next_state, count in next_state_count.items():\n",
        "            total_count = total_count + count\n",
        "        for next_state, count in next_state_count.items():\n",
        "            next_state_count[next_state] = count/total_count"
      ],
      "id": "b970d860-261a-4fe0-926b-7e0af1abd8d5",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b29b91b3-cc67-4986-b351-951611c30c2c"
      },
      "source": [
        "### Depende el modelo del entorno de la policy?"
      ],
      "id": "b29b91b3-cc67-4986-b351-951611c30c2c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d83d3b54-939b-4eb1-95f1-59bf3ea5d305",
        "outputId": "a36623e4-0b5f-490f-ec1f-f8b509b4a353"
      },
      "source": [
        "transition_model[0]"
      ],
      "id": "d83d3b54-939b-4eb1-95f1-59bf3ea5d305",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DOWN': {0: 0.364, 1: 0.324, 4: 0.312},\n",
              " 'LEFT': {0: 0.688, 4: 0.312},\n",
              " 'RIGHT': {0: 0.308, 1: 0.32, 4: 0.372},\n",
              " 'UP': {0: 0.592, 1: 0.408}}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dd98604-1ac8-40a3-bad3-f0d4de8f261f",
        "outputId": "85f79a5b-c60a-4c6c-c147-af012e6aba3c"
      },
      "source": [
        "reward_model[10]"
      ],
      "id": "6dd98604-1ac8-40a3-bad3-f0d4de8f261f",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DOWN': {9: {'count': 194, 'reward': 0.0, 'total_reward': 0},\n",
              "  10: {'count': 168, 'reward': 0.0, 'total_reward': 0},\n",
              "  11: {'count': 138, 'reward': 1.0, 'total_reward': 138.0}},\n",
              " 'LEFT': {6: {'count': 161, 'reward': 0.0, 'total_reward': 0},\n",
              "  9: {'count': 158, 'reward': 0.0, 'total_reward': 0},\n",
              "  10: {'count': 181, 'reward': 0.0, 'total_reward': 0}},\n",
              " 'RIGHT': {6: {'count': 158, 'reward': 0.0, 'total_reward': 0},\n",
              "  10: {'count': 163, 'reward': 0.0, 'total_reward': 0},\n",
              "  11: {'count': 179, 'reward': 1.0, 'total_reward': 179.0}},\n",
              " 'UP': {6: {'count': 178, 'reward': 0.0, 'total_reward': 0},\n",
              "  9: {'count': 169, 'reward': 0.0, 'total_reward': 0},\n",
              "  11: {'count': 153, 'reward': 1.0, 'total_reward': 153.0}}}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c8e4d5c-08ba-4261-8973-c4e588e8fb58",
        "outputId": "732f41f4-6b17-4724-f850-a337d35008da"
      },
      "source": [
        "reward_model[7]"
      ],
      "id": "5c8e4d5c-08ba-4261-8973-c4e588e8fb58",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DOWN': {6: {'count': 194, 'reward': 0.0, 'total_reward': 0},\n",
              "  7: {'count': 154, 'reward': 0.0, 'total_reward': 0},\n",
              "  11: {'count': 152, 'reward': 1.0, 'total_reward': 152.0}},\n",
              " 'LEFT': {3: {'count': 174, 'reward': 0.0, 'total_reward': 0},\n",
              "  6: {'count': 164, 'reward': 0.0, 'total_reward': 0},\n",
              "  11: {'count': 162, 'reward': 1.0, 'total_reward': 162.0}},\n",
              " 'RIGHT': {3: {'count': 176, 'reward': 0.0, 'total_reward': 0},\n",
              "  7: {'count': 163, 'reward': 0.0, 'total_reward': 0},\n",
              "  11: {'count': 161, 'reward': 1.0, 'total_reward': 161.0}},\n",
              " 'UP': {3: {'count': 155, 'reward': 0.0, 'total_reward': 0},\n",
              "  6: {'count': 162, 'reward': 0.0, 'total_reward': 0},\n",
              "  7: {'count': 183, 'reward': 0.0, 'total_reward': 0}}}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c80ffd95-49c1-4238-a614-8113417810ef"
      },
      "source": [
        "# Ejercicio 3: Value iteration"
      ],
      "id": "c80ffd95-49c1-4238-a614-8113417810ef"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "687eddb4-514c-44ee-abb4-3ed14e89d01a"
      },
      "source": [
        "def estimate_V_by_value_iteration(policy, transition_model, reward_model, N=500, number_of_states=12):\n",
        "    Vs = np.zeros(number_of_states)\n",
        "    for i in range(N):\n",
        "        for s, v in enumerate(Vs):\n",
        "            if s in policy:\n",
        "                action = action_to_str[policy[s]]\n",
        "                avg_reward = 0\n",
        "                for next_s, prob in transition_model[s][action].items():\n",
        "                    # Implementar\n",
        "                    reward = reward_model[s][action][next_s]['reward']\n",
        "                    avg_reward = avg_reward + (Vs[next_s] + reward)*prob\n",
        "                Vs[s] = avg_reward\n",
        "    return Vs"
      ],
      "id": "687eddb4-514c-44ee-abb4-3ed14e89d01a",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f17ce8a-0530-4f87-94d1-b5fa26a8a1d1"
      },
      "source": [
        "N = 10\n",
        "policy = policy_1\n",
        "Vs = estimate_V_by_value_iteration(policy, transition_model, reward_model, N=N)"
      ],
      "id": "7f17ce8a-0530-4f87-94d1-b5fa26a8a1d1",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3154247d-5b7b-4af6-9c21-0267ae1d5b5d",
        "outputId": "149e60bf-4e3e-4d06-aee8-c893c3c21f3f"
      },
      "source": [
        "Vs.reshape(3, 4)"
      ],
      "id": "3154247d-5b7b-4af6-9c21-0267ae1d5b5d",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.40554616, 0.65460558, 0.74970397, 0.78866147],\n",
              "       [0.35327082, 0.73563181, 0.84491125, 0.90385623],\n",
              "       [0.        , 0.79002096, 0.91885642, 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bacbd6ce-ba83-42f0-a29b-b0ca6a052c35",
        "outputId": "36dd7eaa-eb31-411d-a908-3c2d1e12bdaf"
      },
      "source": [
        "N = 200\n",
        "Vs_sample_policy = estimate_V_sampling(env, policy, N=N, gamma=gamma)\n",
        "Vs_sample_policy.reshape(3, 4)"
      ],
      "id": "bacbd6ce-ba83-42f0-a29b-b0ca6a052c35",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.79, 1.  , 1.  , 1.  ],\n",
              "       [0.63, 1.  , 1.  , 1.  ],\n",
              "       [0.  , 1.  , 1.  , 0.  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}